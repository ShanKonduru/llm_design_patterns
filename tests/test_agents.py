import unittest
import json
from unittest.mock import patch, MagicMock

# Import the classes to be tested
from src.agents import (
    ConfigLoader,
    Verdict,
    FactualJudgeAgent,
    ClarityJudgeAgent,
    RelevanceJudgeAgent,
    SafetyJudgeAgent,
    ChiefJusticeAgent
)

# Sample case data to be used in tests
SAMPLE_CASE_DATA = {
    "question": "What is the capital of France?",
    "answer": "The capital of France is Paris.",
    "contexts": ["Paris is the capital and most populous city of France."],
    "ground_truth": "The capital of France is Paris."
}

class TestAgentFramework(unittest.TestCase):

    def setUp(self):
        """Set up a fresh ConfigLoader for each test."""
        # Reset the singleton instance to ensure test isolation
        ConfigLoader._instance = None
        self.config_loader = ConfigLoader("agents.json")

    def test_config_loader_singleton(self):
        """Test that ConfigLoader is a singleton."""
        instance1 = ConfigLoader()
        instance2 = ConfigLoader()
        self.assertIs(instance1, instance2, "ConfigLoader should be a singleton")

    def test_config_loader_loads_data(self):
        """Test that the agent configurations are loaded correctly."""
        agent_name = "ClarityJudgeAgent"
        config = self.config_loader.get_agent_config(agent_name)
        self.assertIsNotNone(config)
        self.assertIn("persona_prompt", config)
        self.assertIn("model", config)

    @patch('src.agents.base.LLMFactory.get_llm')
    def test_clarity_judge_agent(self, mock_get_llm):
        """Test the ClarityJudgeAgent's run method."""
        # Arrange: Create a mock LLM that returns a predictable JSON response
        mock_llm = MagicMock()
        mock_response = {
            "final_score": 0.9,
            "verdict_text": "The answer is clear and concise."
        }
        mock_llm.invoke.return_value = json.dumps(mock_response)
        mock_get_llm.return_value = mock_llm

        # Act: Run the agent
        agent = ClarityJudgeAgent(self.config_loader)
        verdict = agent.run(SAMPLE_CASE_DATA)

        # Assert: Check that the verdict is parsed correctly
        self.assertIsInstance(verdict, Verdict)
        self.assertEqual(verdict.judge_name, "ClarityJudgeAgent")
        self.assertEqual(verdict.score, 0.9)
        self.assertEqual(verdict.verdict, "The answer is clear and concise.")
        mock_llm.invoke.assert_called_once()

    @patch('src.agents.factual_judge.RagasEvaluator')
    @patch('src.agents.base.LLMFactory.get_llm')
    def test_factual_judge_agent(self, mock_get_llm, mock_ragas_evaluator):
        """Test the FactualJudgeAgent's run method, mocking the Ragas tool."""
        # Arrange: Mock the LLM
        mock_llm = MagicMock()
        mock_response = {
            "final_score": 0.95,
            "verdict_text": "The answer is factually consistent."
        }
        mock_llm.invoke.return_value = json.dumps(mock_response)
        mock_get_llm.return_value = mock_llm

        # Arrange: Mock the RagasEvaluator tool
        mock_ragas_instance = mock_ragas_evaluator.return_value
        mock_ragas_instance.evaluate_faithfulness.return_value = {"faithfulness": 1.0}
        mock_ragas_instance.evaluate_answer_correctness.return_value = {"answer_correctness": 0.9}

        # Act: Run the agent
        agent = FactualJudgeAgent(self.config_loader)
        verdict = agent.run(SAMPLE_CASE_DATA)

        # Assert: Check the verdict and that the tools were used
        self.assertIsInstance(verdict, Verdict)
        self.assertEqual(verdict.score, 0.95)
        self.assertEqual(verdict.verdict, "The answer is factually consistent.")
        self.assertEqual(verdict.metrics["faithfulness"], 1.0)
        self.assertEqual(verdict.metrics["answer_correctness"], 0.9)
        mock_ragas_instance.evaluate_faithfulness.assert_called_once()
        mock_ragas_instance.evaluate_answer_correctness.assert_called_once()
        mock_llm.invoke.assert_called_once()

    @patch('src.agents.chief_justice.FactualJudgeAgent')
    @patch('src.agents.chief_justice.ClarityJudgeAgent')
    @patch('src.agents.chief_justice.RelevanceJudgeAgent')
    @patch('src.agents.chief_justice.SafetyJudgeAgent')
    @patch('src.agents.base.LLMFactory.get_llm')
    def test_chief_justice_agent(self, mock_get_llm, mock_safety, mock_relevance, mock_clarity, mock_factual):
        """Test the ChiefJusticeAgent's orchestration logic."""
        # Arrange: Mock the Chief Justice's own LLM
        mock_llm = MagicMock()
        mock_response = {
            "final_score": 0.875, # This would be generated by the LLM, but we test our own calculation
            "verdict_text": "A synthesized final judgment."
        }
        mock_llm.invoke.return_value = json.dumps(mock_response)
        mock_get_llm.return_value = mock_llm

        # Arrange: Mock the individual judge agents to return predictable verdicts
        mock_factual.return_value.run.return_value = Verdict("FactualJudgeAgent", 1.0, "Factually perfect.")
        mock_clarity.return_value.run.return_value = Verdict("ClarityJudgeAgent", 0.8, "Quite clear.")
        mock_relevance.return_value.run.return_value = Verdict("RelevanceJudgeAgent", 0.9, "Very relevant.")
        mock_safety.return_value.run.return_value = Verdict("SafetyJudgeAgent", 0.8, "Seems safe.")

        # Act: Run the Chief Justice agent
        chief_justice = ChiefJusticeAgent(self.config_loader)
        final_verdict = chief_justice.run(SAMPLE_CASE_DATA)

        # Assert: Check the final verdict and orchestration
        self.assertIsInstance(final_verdict, Verdict)
        self.assertEqual(final_verdict.judge_name, "ChiefJusticeAgent")
        # The final score should be the average of the mocked scores (1.0 + 0.8 + 0.9 + 0.8) / 4 = 0.875
        self.assertAlmostEqual(final_verdict.score, 0.875)
        self.assertEqual(final_verdict.verdict, "A synthesized final judgment.")
        
        # Check that each judge was called
        mock_factual.return_value.run.assert_called_once_with(SAMPLE_CASE_DATA)
        mock_clarity.return_value.run.assert_called_once_with(SAMPLE_CASE_DATA)
        mock_relevance.return_value.run.assert_called_once_with(SAMPLE_CASE_DATA)
        mock_safety.return_value.run.assert_called_once_with(SAMPLE_CASE_DATA)

        # Check that the Chief Justice's LLM was called to synthesize the results
        mock_llm.invoke.assert_called_once()

    @patch('src.agents.base.LLMFactory.get_llm')
    def test_relevance_judge_agent(self, mock_get_llm):
        """Test the RelevanceJudgeAgent's run method."""
        # Arrange: Create a mock LLM
        mock_llm = MagicMock()
        mock_response = {
            "final_score": 0.85,
            "verdict_text": "The answer is highly relevant."
        }
        mock_llm.invoke.return_value = json.dumps(mock_response)
        mock_get_llm.return_value = mock_llm

        # Act: Run the agent
        agent = RelevanceJudgeAgent(self.config_loader)
        verdict = agent.run(SAMPLE_CASE_DATA)

        # Assert: Check the verdict
        self.assertIsInstance(verdict, Verdict)
        self.assertEqual(verdict.judge_name, "RelevanceJudgeAgent")
        self.assertEqual(verdict.score, 0.85)
        self.assertEqual(verdict.verdict, "The answer is highly relevant.")
        mock_llm.invoke.assert_called_once()

    @patch('src.agents.base.LLMFactory.get_llm')
    def test_safety_judge_agent(self, mock_get_llm):
        """Test the SafetyJudgeAgent's run method."""
        # Arrange: Create a mock LLM
        mock_llm = MagicMock()
        mock_response = {
            "final_score": 1.0,
            "verdict_text": "The answer is perfectly safe."
        }
        mock_llm.invoke.return_value = json.dumps(mock_response)
        mock_get_llm.return_value = mock_llm

        # Act: Run the agent
        agent = SafetyJudgeAgent(self.config_loader)
        verdict = agent.run(SAMPLE_CASE_DATA)

        # Assert: Check the verdict
        self.assertIsInstance(verdict, Verdict)
        self.assertEqual(verdict.judge_name, "SafetyJudgeAgent")
        self.assertEqual(verdict.score, 1.0)
        self.assertEqual(verdict.verdict, "The answer is perfectly safe.")
        mock_llm.invoke.assert_called_once()

    @patch('src.agents.chief_justice.FactualJudgeAgent')
    @patch('src.agents.chief_justice.ClarityJudgeAgent')
    @patch('src.agents.chief_justice.RelevanceJudgeAgent')
    @patch('src.agents.chief_justice.SafetyJudgeAgent')
    @patch('src.agents.base.LLMFactory.get_llm')
    def test_chief_justice_no_verdicts(self, mock_get_llm, mock_safety, mock_relevance, mock_clarity, mock_factual):
        """Test ChiefJusticeAgent when the jury returns no verdicts."""
        # Arrange: Mock the Chief Justice's own LLM, it shouldn't be called.
        mock_llm = MagicMock()
        mock_get_llm.return_value = mock_llm

        # Arrange: Mock the run method of all judge instances to return None
        mock_factual.return_value.run.return_value = None
        mock_clarity.return_value.run.return_value = None
        mock_relevance.return_value.run.return_value = None
        mock_safety.return_value.run.return_value = None

        # Act
        chief_justice = ChiefJusticeAgent(self.config_loader)
        final_verdict = chief_justice.run(SAMPLE_CASE_DATA)

        # Assert
        self.assertIsNone(final_verdict, "Should return None when no judges provide a verdict")
        mock_llm.invoke.assert_not_called()

    def test_verdicts_to_dataframe(self):
        """Test the verdicts_to_dataframe helper method."""
        # Arrange
        chief_justice = ChiefJusticeAgent(self.config_loader)
        
        # Create a sample final verdict with nested verdicts
        individual_verdicts = [
            Verdict("FactualJudgeAgent", 1.0, "Factually perfect.", metrics={"faithfulness": 1.0}),
            Verdict("ClarityJudgeAgent", 0.8, "Quite clear.")
        ]
        final_verdict = Verdict(
            "ChiefJusticeAgent", 
            0.9, 
            "A synthesized judgment.", 
            metrics={"individual_verdicts": individual_verdicts}
        )

        # Act
        df = chief_justice.verdicts_to_dataframe(final_verdict)

        # Assert
        self.assertEqual(len(df), 3) # 2 judges + 1 chief justice
        self.assertIn("judge", df.columns)
        self.assertIn("score", df.columns)
        self.assertIn("verdict", df.columns)
        self.assertIn("faithfulness", df.columns)
        self.assertEqual(df.iloc[0]['judge'], "FactualJudgeAgent")
        self.assertEqual(df.iloc[2]['judge'], "ChiefJusticeAgent")
        self.assertEqual(df.iloc[2]['score'], 0.9)

    @patch('src.agents.base.LLMFactory.get_llm')
    def test_judge_handles_malformed_json(self, mock_get_llm):
        """Test that a judge agent handles a malformed JSON response from the LLM."""
        # Arrange
        mock_llm = MagicMock()
        # Return a non-JSON string
        mock_llm.invoke.return_value = "This is not JSON."
        mock_get_llm.return_value = mock_llm

        # Act
        agent = ClarityJudgeAgent(self.config_loader)
        verdict = agent.run(SAMPLE_CASE_DATA)

        # Assert
        self.assertIsInstance(verdict, Verdict)
        self.assertEqual(verdict.score, 0.0)
        self.assertIn("Failed to generate a valid verdict", verdict.verdict)

    @patch("builtins.open", new_callable=unittest.mock.mock_open, read_data="[invalid json")
    def test_config_loader_bad_json(self, mock_open):
        """Test ConfigLoader with a malformed JSON file."""
        ConfigLoader._instance = None
        with self.assertRaises(json.JSONDecodeError):
            ConfigLoader("bad_config.json")

    @patch("builtins.open", side_effect=FileNotFoundError)
    def test_config_loader_file_not_found(self, mock_open):
        """Test ConfigLoader when the config file is not found."""
        ConfigLoader._instance = None
        with self.assertRaises(FileNotFoundError):
            ConfigLoader("non_existent_file.json")

    @patch('src.agents.chief_justice.FactualJudgeAgent')
    @patch('src.agents.chief_justice.ClarityJudgeAgent')
    @patch('src.agents.chief_justice.RelevanceJudgeAgent')
    @patch('src.agents.chief_justice.SafetyJudgeAgent')
    @patch('src.agents.base.LLMFactory.get_llm')
    def test_chief_justice_handles_malformed_json(self, mock_get_llm, mock_safety, mock_relevance, mock_clarity, mock_factual):
        """Test that the ChiefJusticeAgent handles a malformed JSON response."""
        # Arrange
        mock_llm = MagicMock()
        mock_llm.invoke.return_value = "This is not JSON."
        mock_get_llm.return_value = mock_llm

        # Mock the jury to return a valid verdict from at least one judge
        mock_factual.return_value.run.return_value = Verdict("FactualJudgeAgent", 1.0, "Perfect.")
        mock_clarity.return_value.run.return_value = None
        mock_relevance.return_value.run.return_value = None
        mock_safety.return_value.run.return_value = None

        chief_justice = ChiefJusticeAgent(self.config_loader)
        
        # Act
        verdict = chief_justice.run(SAMPLE_CASE_DATA)

        # Assert
        self.assertIsInstance(verdict, Verdict)
        self.assertEqual(verdict.score, 0.0) # Score is 0.0 on failure
        self.assertIn("Failed to generate a valid final judgment", verdict.verdict)

    @patch('src.agents.factual_judge.RagasEvaluator')
    @patch('src.agents.base.LLMFactory.get_llm')
    def test_factual_judge_handles_ragas_error(self, mock_get_llm, mock_ragas_evaluator):
        """Test that FactualJudgeAgent handles an exception from the Ragas tool."""
        # Arrange
        mock_get_llm.return_value = MagicMock()
        mock_ragas_instance = mock_ragas_evaluator.return_value
        mock_ragas_instance.evaluate_faithfulness.side_effect = Exception("Ragas tool failed")

        # Act
        agent = FactualJudgeAgent(self.config_loader)
        verdict = agent.run(SAMPLE_CASE_DATA)

        # Assert
        self.assertIsNone(verdict, "Should return None when Ragas tool fails")

    @patch('src.agents.base.LLMFactory.get_llm')
    def test_relevance_judge_handles_malformed_json(self, mock_get_llm):
        """Test that RelevanceJudgeAgent handles a malformed JSON response."""
        # Arrange
        mock_llm = MagicMock()
        mock_llm.invoke.return_value = "This is not JSON."
        mock_get_llm.return_value = mock_llm

        # Act
        agent = RelevanceJudgeAgent(self.config_loader)
        verdict = agent.run(SAMPLE_CASE_DATA)

        # Assert
        self.assertIsInstance(verdict, Verdict)
        self.assertEqual(verdict.score, 0.0)
        self.assertIn("Failed to generate a valid verdict", verdict.verdict)

    @patch('src.agents.base.LLMFactory.get_llm')
    def test_safety_judge_handles_malformed_json(self, mock_get_llm):
        """Test that SafetyJudgeAgent handles a malformed JSON response."""
        # Arrange
        mock_llm = MagicMock()
        mock_llm.invoke.return_value = "This is not JSON."
        mock_get_llm.return_value = mock_llm

        # Act
        agent = SafetyJudgeAgent(self.config_loader)
        verdict = agent.run(SAMPLE_CASE_DATA)

        # Assert
        self.assertIsInstance(verdict, Verdict)
        self.assertEqual(verdict.score, 0.0)
        self.assertIn("Failed to generate a valid verdict", verdict.verdict)

    @patch('src.agents.base.LLMFactory.get_llm')
    def test_judge_handles_plain_json_response(self, mock_get_llm):
        """Test that a judge agent handles a plain JSON response without markdown."""
        # Arrange
        mock_llm = MagicMock()
        mock_response = {
            "final_score": 0.7,
            "verdict_text": "This is a plain JSON response."
        }
        # Return a plain JSON string, not wrapped in ```json
        mock_llm.invoke.return_value = json.dumps(mock_response)
        mock_get_llm.return_value = mock_llm

        # Act
        agent = ClarityJudgeAgent(self.config_loader)
        verdict = agent.run(SAMPLE_CASE_DATA)

        # Assert
        self.assertIsInstance(verdict, Verdict)
        self.assertEqual(verdict.score, 0.7)
        self.assertEqual(verdict.verdict, "This is a plain JSON response.")

if __name__ == '__main__':
    unittest.main()
